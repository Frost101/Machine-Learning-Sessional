{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixed seed for reproducibility\n",
    "import numpy as np\n",
    "np.random.seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the FashionMNIST dataset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "transforms = transforms.ToTensor()\n",
    "\n",
    "# Load the training data\n",
    "train_dataset = datasets.FashionMNIST('./data', download=True, train=True, transform=transforms)\n",
    "\n",
    "# Load the test data\n",
    "test_dataset = datasets.FashionMNIST('./data', download=True, train=False, transform=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the data\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=5, sharex=True, sharey=True, figsize=(20, 5))\n",
    "for i in range(10):\n",
    "    ax = axes[i // 5, i % 5]\n",
    "    ax.imshow(train_dataset.data[i], cmap='gray')\n",
    "    ax.set_title(train_dataset.classes[train_dataset.targets[i]])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to calculate accuracy, precision, recall, and F1 score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "\n",
    "    def forward(self, input):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, output_grad, learning_rate):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "\n",
    "class Activation(Layer):\n",
    "    def __init__(self):\n",
    "        super(Activation, self).__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def backward(self, output_grad, learning_rate):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "\n",
    "class Regularization(Layer):\n",
    "    def __init__(self):\n",
    "        super(Regularization, self).__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def backward(self, output_grad, learning_rate):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def parameters(self):\n",
    "        return []\n",
    "    \n",
    "\n",
    "class Optimizer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def step(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "\n",
    "class Normalization(Layer):\n",
    "    def __init__(self):\n",
    "        super(Normalization, self).__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def backward(self, output_grad, learning_rate):\n",
    "        raise NotImplementedError \n",
    "    \n",
    "\n",
    "class Loss:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def backward(self):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Dense, self).__init__()\n",
    "        # Xavier initialization\n",
    "        # weight dimension = (input_dim, output_dim)\n",
    "        self.weight = np.random.randn(input_dim, output_dim) * np.sqrt(1 / input_dim)\n",
    "\n",
    "        # bias dimension = (1, output_dim)\n",
    "        self.bias = np.zeros((1, output_dim))\n",
    "\n",
    "        # Initialize the gradients\n",
    "        # weight gradient dimension = (input_dim, output_dim)\n",
    "        self.weight_grad = np.zeros_like(self.weight)\n",
    "\n",
    "        # bias gradient dimension = (1, output_dim)\n",
    "        self.bias_grad = np.zeros_like(self.bias)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # input dimension = (batch_size, features)\n",
    "        self.input = X\n",
    "\n",
    "        # input dimension = (batch_size, features)\n",
    "        # weight dimension = (features, output_dim)\n",
    "        # output dimension = (batch_size, output_dim)\n",
    "        self.output = np.dot(self.input, self.weight) + self.bias\n",
    "\n",
    "        return self.output\n",
    "    \n",
    "\n",
    "    def update_params(self, lr):\n",
    "        self.weight -= lr * self.weight_grad\n",
    "        self.bias -= lr * self.bias_grad\n",
    "    \n",
    "    def backward(self, grad_in, lr):\n",
    "        # grad_in dimension = (batch_size, output_dim)\n",
    "\n",
    "        # input dimension = (batch_size, input_dim)\n",
    "        # weight dimension = (input_dim, output_dim)\n",
    "        # grad_out dimension = (batch_size, input_dim)\n",
    "        grad_out = np.dot(grad_in, self.weight.T)\n",
    "\n",
    "        # input dimension = (batch_size, input_dim)\n",
    "        # grad_in dimension = (batch_size, output_dim)\n",
    "        # weight_grad dimension = (input_dim, output_dim)\n",
    "        self.weight_grad = np.dot(self.input.T, grad_in)\n",
    "\n",
    "        # bias_grad dimension = (1, output_dim)\n",
    "        self.bias_grad = np.sum(grad_in, axis=0, keepdims=True)\n",
    "\n",
    "        # Update the weights\n",
    "        # self.update_params(lr)\n",
    "\n",
    "        return grad_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RELU Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Activation):\n",
    "    def __init__(self):\n",
    "        super(ReLU, self).__init__()\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.input = X\n",
    "        return np.maximum(0, X)\n",
    "    \n",
    "    def backward(self, grad_in, lr):\n",
    "        return grad_in * (self.input > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SoftMax Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftMax(Activation):\n",
    "    def __init__(self):\n",
    "        super(SoftMax, self).__init__()\n",
    "        self.output = None\n",
    "        self.input = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.input = X\n",
    "        \n",
    "        X_max = np.max(X, axis=1, keepdims=True)\n",
    "        stab = X - X_max\n",
    "        exps = np.exp(X - X_max)\n",
    "        self.output = exps / (np.sum(exps, axis=1, keepdims=True) + 1e-10)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, grad_in, lr):\n",
    "        return grad_in\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout(Regularization):\n",
    "    def __init__(self, dropout_rate):\n",
    "        super(Dropout, self).__init__()\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.mask = None\n",
    "        self.train = True\n",
    "\n",
    "    def forward(self, X):\n",
    "        if self.train:\n",
    "            self.mask = np.random.binomial(1, 1 - self.dropout_rate, size=X.shape) / (1 - self.dropout_rate)\n",
    "            return X * self.mask\n",
    "        return X\n",
    "    \n",
    "    def backward(self, grad_in, lr):\n",
    "        return grad_in * self.mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batchnorm(Normalization):\n",
    "    def __init__(self, input_dim):\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        # gamma dimension = (1, input_dim)\n",
    "        self.weight = np.random.randn(1, input_dim) * np.sqrt(1 / input_dim)\n",
    "\n",
    "        # beta dimension = (1, input_dim)\n",
    "        self.bias = np.zeros((1, input_dim))\n",
    "\n",
    "        # Initialize the gradients\n",
    "        # gamma gradient dimension = (1, input_dim)\n",
    "        self.weight_grad = np.zeros_like(self.weight)\n",
    "\n",
    "        # beta gradient dimension = (1, input_dim)\n",
    "        self.bias_grad = np.zeros_like(self.bias)\n",
    "\n",
    "        self.input = None\n",
    "        self.normalized_input = None\n",
    "        self.miu = None\n",
    "        self.variance = None\n",
    "        self.epsilon = 1e-7\n",
    "        self.output = None\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.input = X\n",
    "\n",
    "        # input dimension = (batch_size, input_dim)\n",
    "        # miu dimension = (1, input_dim)\n",
    "        self.miu = np.mean(self.input, axis=0, keepdims=True)\n",
    "        \n",
    "        # input dimension = (batch_size, input_dim)\n",
    "        # variance dimension = (1, input_dim)\n",
    "        self.variance = np.var(self.input, axis=0, keepdims=True)\n",
    "\n",
    "        # input dimension = (batch_size, input_dim)\n",
    "        # normalized_input dimension = (batch_size, input_dim)\n",
    "        self.normalized_input = (self.input - self.miu) / np.sqrt(self.variance + self.epsilon)\n",
    "\n",
    "        # input dimension = (batch_size, input_dim)\n",
    "        # gamma dimension = (1, input_dim)\n",
    "        # beta dimension = (1, input_dim)\n",
    "        # output dimension = (batch_size, input_dim)\n",
    "        self.output = self.weight * self.normalized_input + self.bias\n",
    "\n",
    "        return self.output\n",
    "    \n",
    "\n",
    "    def backward(self, grad_in, lr):\n",
    "        # dbias shape = (1, input_dim)\n",
    "        dbias = np.sum(grad_in, axis=0, keepdims=True)\n",
    "\n",
    "        # dgamma shape = (1, input_dim)\n",
    "        dweights = np.sum(grad_in * self.normalized_input, axis=0, keepdims=True)\n",
    "\n",
    "        # dnormalized_input shape = (batch_size, input_dim)\n",
    "        dnormalized_input = grad_in * self.weight\n",
    "\n",
    "        # dvairance shape = (1, input_dim)\n",
    "        dvariance = np.sum(dnormalized_input * (self.input - self.miu) * -0.5 * np.power(self.variance + self.epsilon, -1.5), axis=0, keepdims=True)\n",
    "\n",
    "        # dmiu shape = (1, input_dim)\n",
    "        dmiu = np.sum(dnormalized_input * -1 / np.sqrt(self.variance + self.epsilon), axis=0, keepdims=True) + dvariance * np.mean(-2 * (self.input - self.miu), axis=0, keepdims=True)\n",
    "\n",
    "        # grad_out shape = (batch_size, input_dim)\n",
    "        grad_out = dnormalized_input / np.sqrt(self.variance + self.epsilon) + dvariance * 2 * (self.input - self.miu) / self.input.shape[0] + dmiu / self.input.shape[0]\n",
    "\n",
    "        self.weight_grad = dweights\n",
    "        self.bias_grad = dbias\n",
    "\n",
    "        return grad_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adam Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam(Optimizer):\n",
    "    def __init__(self, layers, lr=0.001, beta1=0.9, beta2=0.999):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = 1e-8\n",
    "        self.layers = layers\n",
    "        \n",
    "        # if the layer has weights and biases, initialize the m_t and v_t for weights and biases \n",
    "        # if not initialize them to None\n",
    "        self.m_t_weight = [None] * len(self.layers)\n",
    "        self.v_t_weight = [None] * len(self.layers)\n",
    "        self.m_t_bias = [None] * len(self.layers)\n",
    "        self.v_t_bias = [None] * len(self.layers)\n",
    "\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if hasattr(layer, 'weight'):\n",
    "                self.m_t_weight[i] = np.zeros_like(layer.weight)\n",
    "                self.v_t_weight[i] = np.zeros_like(layer.weight)\n",
    "            if hasattr(layer, 'bias'):\n",
    "                self.m_t_bias[i] = np.zeros_like(layer.bias)\n",
    "                self.v_t_bias[i] = np.zeros_like(layer.bias)\n",
    "        self.t = 0 \n",
    "\n",
    "    def step(self):\n",
    "        self.t += 1\n",
    "        for i, layer in enumerate(self.layers):\n",
    "           \n",
    "            if hasattr(layer, 'weight'):\n",
    "                self.m_t_weight[i] = self.beta1 * self.m_t_weight[i] + (1 - self.beta1) * layer.weight_grad\n",
    "                self.v_t_weight[i] = self.beta2 * self.v_t_weight[i] + (1 - self.beta2) * layer.weight_grad ** 2\n",
    "\n",
    "                m_t_weight_hat = self.m_t_weight[i] / (1 - self.beta1 ** self.t)\n",
    "                v_t_weight_hat = self.v_t_weight[i] / (1 - self.beta2 ** self.t)\n",
    "\n",
    "                layer.weight -= self.lr * m_t_weight_hat / (np.sqrt(v_t_weight_hat) + self.epsilon)\n",
    "\n",
    "            if hasattr(layer, 'bias'):\n",
    "                self.m_t_bias[i] = self.beta1 * self.m_t_bias[i] + (1 - self.beta1) * layer.bias_grad\n",
    "                self.v_t_bias[i] = self.beta2 * self.v_t_bias[i] + (1 - self.beta2) * layer.bias_grad ** 2\n",
    "\n",
    "                m_t_bias_hat = self.m_t_bias[i] / (1 - self.beta1 ** self.t)\n",
    "                v_t_bias_hat = self.v_t_bias[i] / (1 - self.beta2 ** self.t)\n",
    "\n",
    "                layer.bias -= self.lr * m_t_bias_hat / (np.sqrt(v_t_bias_hat) + self.epsilon)\n",
    "\n",
    "    def save_state_dict(self, state_dict):\n",
    "       \n",
    "        state_dict['m_t_weight'] = self.m_t_weight\n",
    "        state_dict['v_t_weight'] = self.v_t_weight\n",
    "        state_dict['m_t_bias'] = self.m_t_bias\n",
    "        state_dict['v_t_bias'] = self.v_t_bias\n",
    "        state_dict['t'] = self.t\n",
    "        state_dict['lr'] = self.lr\n",
    "        state_dict['beta1'] = self.beta1\n",
    "        state_dict['beta2'] = self.beta2\n",
    "        state_dict['epsilon'] = self.epsilon\n",
    "\n",
    "        return state_dict\n",
    "    \n",
    "    def load_state_dict(self, state_dict):\n",
    "        \n",
    "        self.m_t_weight = state_dict['m_t_weight']\n",
    "        self.v_t_weight = state_dict['v_t_weight']\n",
    "        self.m_t_bias = state_dict['m_t_bias']\n",
    "        self.v_t_bias = state_dict['v_t_bias']\n",
    "        self.t = state_dict['t']\n",
    "        self.lr = state_dict['lr']\n",
    "        self.beta1 = state_dict['beta1']\n",
    "        self.beta2 = state_dict['beta2']\n",
    "        self.epsilon = state_dict['epsilon']\n",
    "\n",
    "        return self\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricalCrossEntropyLoss(Loss):\n",
    "    def __init__(self):\n",
    "        super(CategoricalCrossEntropyLoss, self).__init__()\n",
    "        self.loss = None\n",
    "        self.labels = None\n",
    "        self.pred = None\n",
    "\n",
    "    def forward(self, pred, labels):\n",
    "        # one-hot encode the labels\n",
    "        self.labels = np.eye(pred.shape[1])[labels]\n",
    "        \n",
    "        # pred dimension = (batch_size, num_classes)\n",
    "        # labels dimension = (batch_size, num_classes)\n",
    "        self.pred = pred\n",
    "        self.loss = -np.sum(self.labels * np.log(pred + 1e-10)) / pred.shape[0]\n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self):\n",
    "        return self.pred - self.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNeuralNetwork:\n",
    "    def __init__(self, layers, loss_fn, optimizer):\n",
    "        self.layers = layers\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def forward(self, X):\n",
    "        for layer in self.layers:\n",
    "            X = layer.forward(X)\n",
    "        return X\n",
    "    \n",
    "    def backward(self, grad_in, lr):\n",
    "        for layer in reversed(self.layers):\n",
    "            grad_in = layer.backward(grad_in, lr)\n",
    "\n",
    "    def train(self, X, labels, lr):\n",
    "        # Forward pass \n",
    "        # reshape the input\n",
    "        X = X.reshape(X.shape[0], -1)\n",
    "       \n",
    "        output = self.forward(X)\n",
    "        \n",
    "        loss = self.loss_fn.forward(output, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        grad = self.loss_fn.backward()\n",
    "        self.backward(grad, lr)\n",
    "\n",
    "        # Update the weights\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Forward pass\n",
    "        # reshape the input\n",
    "        X = X.reshape(X.shape[0], -1)\n",
    "        output = self.forward(X)\n",
    "        return output \n",
    "    \n",
    "    def eval_mode(self):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, Dropout):\n",
    "                layer.train = False\n",
    "\n",
    "    def train_mode(self):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, Dropout):\n",
    "                layer.train = True\n",
    "\n",
    "    def save_model(self, path):\n",
    "        # save the weights and biases of the model\n",
    "        state_dict = {}\n",
    "\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if hasattr(layer, 'weight'):\n",
    "                state_dict[f'weight_{i}'] = layer.weight\n",
    "            if hasattr(layer, 'bias'):\n",
    "                state_dict[f'bias_{i}'] = layer.bias\n",
    "\n",
    "        # save the state of the optimizer\n",
    "        state_dict = self.optimizer.save_state_dict(state_dict)\n",
    "\n",
    "       \n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(state_dict, f)\n",
    "\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def load_model(path, layers, loss_fn, optimizer):\n",
    "        # load the weights and biases of the model\n",
    "        with open(path, 'rb') as f:\n",
    "            state_dict = pickle.load(f)\n",
    "\n",
    "        model = MyNeuralNetwork(layers, loss_fn, optimizer)\n",
    "        for i, layer in enumerate(model.layers):\n",
    "            if hasattr(layer, 'weight'):\n",
    "                layer.weight = state_dict[f'weight_{i}']\n",
    "            if hasattr(layer, 'bias'):\n",
    "                layer.bias = state_dict[f'bias_{i}']\n",
    "\n",
    "        # load the state of the optimizer\n",
    "        model.optimizer = model.optimizer.load_state_dict(state_dict)\n",
    "        model.optimizer.layers = model.layers\n",
    "        \n",
    "        return model\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tqdm\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(train_dataset, test_dataset):\n",
    "    train_data = train_dataset.data.numpy()\n",
    "    train_labels = train_dataset.targets.numpy()\n",
    "\n",
    "    # split\n",
    "    train_data, val_data, train_labels, val_labels = train_test_split(train_data, train_labels, test_size=0.2)\n",
    "\n",
    "    test_data = test_dataset.data.numpy()\n",
    "    test_labels = test_dataset.targets.numpy()\n",
    "\n",
    "    # Normalize the data\n",
    "    train_data = train_data / 255.0\n",
    "    val_data = val_data / 255.0\n",
    "    test_data = test_data / 255.0 \n",
    "\n",
    "    # shuffle the data\n",
    "    idx = np.random.permutation(len(train_data))\n",
    "    train_data, train_labels = train_data[idx], train_labels[idx]\n",
    "\n",
    "    idx = np.random.permutation(len(val_data))\n",
    "    val_data, val_labels = val_data[idx], val_labels[idx]\n",
    "\n",
    "    idx = np.random.permutation(len(test_data))\n",
    "    test_data, test_labels = test_data[idx], test_labels[idx]\n",
    "\n",
    "    return train_data, val_data, test_data, train_labels, val_labels, test_labels\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(layers ,train_data, train_labels, val_data, val_labels ,learning_rate, nepochs, batch_size):\n",
    "    input_dim = train_data.shape[1] * train_data.shape[1]\n",
    "    output_dim = len(np.unique(train_labels))\n",
    "   \n",
    "    loss_fn = CategoricalCrossEntropyLoss()\n",
    "\n",
    "    optimizer = Adam(layers, lr=learning_rate)\n",
    "\n",
    "    \n",
    "    model = MyNeuralNetwork(layers, loss_fn, optimizer)\n",
    "\n",
    "    \n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    train_acc = []\n",
    "    val_acc = []\n",
    "    val_macro_f1 = []\n",
    "\n",
    "    \n",
    "\n",
    "    for epoch in tqdm(range(nepochs)):\n",
    "        model.train_mode()\n",
    "        epoch_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        \n",
    "        idx = np.random.permutation(len(train_data))\n",
    "        train_data, train_labels = train_data[idx], train_labels[idx]\n",
    "\n",
    "        no_of_iterations = len(train_data) // batch_size\n",
    "        for i in range(0, no_of_iterations):\n",
    "            X = train_data[i * batch_size: (i + 1) * batch_size]\n",
    "            labels = train_labels[i * batch_size: (i + 1) * batch_size]\n",
    "            loss = model.train(X, labels, learning_rate)\n",
    "            epoch_loss += loss\n",
    "            pred = np.argmax(model.predict(X), axis=1)\n",
    "            correct += np.sum(pred == labels)\n",
    "            total += len(labels)\n",
    "        train_loss.append(epoch_loss / no_of_iterations)\n",
    "        train_acc.append(correct / total)\n",
    "\n",
    "        model.eval_mode()\n",
    "        epoch_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        f1 = 0 \n",
    "        conf_matrix = np.zeros((output_dim, output_dim))\n",
    "        no_of_iterations = len(val_data) // batch_size\n",
    "        for i in range(0, no_of_iterations):\n",
    "            X = val_data[i * batch_size: (i + 1) * batch_size]\n",
    "            labels = val_labels[i * batch_size: (i + 1) * batch_size]\n",
    "            loss = model.train(X, labels, learning_rate)\n",
    "            epoch_loss += loss\n",
    "            pred = np.argmax(model.predict(X), axis=1)\n",
    "            correct += np.sum(pred == labels)\n",
    "            total += len(labels)\n",
    "            f1 += f1_score(labels, pred, average='macro')\n",
    "            conf_matrix += confusion_matrix(labels, pred, labels=np.unique(train_labels))\n",
    "\n",
    "        val_loss.append(epoch_loss / no_of_iterations)\n",
    "        val_acc.append(correct / total)\n",
    "        val_macro_f1.append(f1 / no_of_iterations)\n",
    "        print(f\"Epoch: {epoch + 1}, Train Loss: {train_loss[-1]:.4f}, Val Loss: {val_loss[-1]:.4f}, Train Acc: {train_acc[-1]:.4f}, Val Acc: {val_acc[-1]:.4f}, Val Macro F1: {val_macro_f1[-1]:.4f}\")\n",
    "\n",
    "        # implement learning rate scheduler based on the validation loss\n",
    "        if epoch > 0:\n",
    "            if val_loss[-1] >= val_loss[-2]:\n",
    "                learning_rate = learning_rate * 0.5\n",
    "                optimizer.lr = learning_rate\n",
    "                print(f\"Learning rate reduced to {learning_rate}\")\n",
    "\n",
    "        # early stopping\n",
    "        if epoch > 10:\n",
    "            if val_loss[-1] >= val_loss[-2] and val_loss[-2] >= val_loss[-3] and val_loss[-3] >= val_loss[-4]:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "    # save the model\n",
    "    model.save_model('model/model.pkl')\n",
    "    return train_loss, val_loss, train_acc, val_acc, val_macro_f1, conf_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tester(layers, path, test_data, test_labels, batch_size):\n",
    "    model = MyNeuralNetwork.load_model(path, layers, CategoricalCrossEntropyLoss(), Adam(layers, lr=0.001))\n",
    "    \n",
    "    model.eval_mode()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    predictions = []\n",
    "    targets = []\n",
    "\n",
    "    no_of_iterations = len(test_data) // batch_size\n",
    "    for i in range(0, no_of_iterations):\n",
    "        X = test_data[i * batch_size: (i + 1) * batch_size]\n",
    "        labels = test_labels[i * batch_size: (i + 1) * batch_size]\n",
    "        pred = np.argmax(model.predict(X), axis=1)\n",
    "        predictions.extend(pred.tolist())\n",
    "        targets.extend(labels.tolist())\n",
    "        correct += np.sum(pred == labels)\n",
    "        total += len(labels)\n",
    "    \n",
    "\n",
    "    accuracy = correct / total\n",
    "    precision = precision_score(targets, predictions, average='macro')\n",
    "    recall = recall_score(targets, predictions, average='macro')\n",
    "    f1 = f1_score(targets, predictions, average='macro')\n",
    "    conf_matrix = confusion_matrix(targets, predictions)\n",
    "\n",
    "    print(f'Test Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}')\n",
    "\n",
    "    return accuracy, precision, recall, f1, conf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess and load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the data\n",
    "train_data, val_data, test_data, train_labels, val_labels, test_labels = preprocess_data(train_dataset, test_dataset)\n",
    "\n",
    "input_dim = train_data.shape[1] * train_data.shape[1]\n",
    "output_dim = len(np.unique(train_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [\n",
    "    Dense(input_dim, 512),\n",
    "    Batchnorm(512),\n",
    "    ReLU(),\n",
    "    Dropout(0.5),\n",
    "    Dense(512, 256),\n",
    "    Batchnorm(256),\n",
    "    ReLU(),\n",
    "    Dropout(0.5),\n",
    "    Dense(256, output_dim),\n",
    "    SoftMax()\n",
    "]\n",
    "# train the model\n",
    "train_loss, val_loss, train_acc, val_acc, val_macro_f1, val_conf_matrix = trainer(\n",
    "    layers=layers,\n",
    "    train_data=train_data,\n",
    "    train_labels=train_labels,\n",
    "    val_data=val_data,\n",
    "    val_labels=val_labels,\n",
    "    learning_rate=0.0005,\n",
    "    nepochs=60,\n",
    "    batch_size=64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the loss , accuracy and f1 score\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(train_loss, label='Train Loss')\n",
    "plt.plot(val_loss, label='Val Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(train_acc, label='Train Accuracy')\n",
    "plt.plot(val_acc, label='Val Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(val_macro_f1, label='Val Macro F1')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the validation confusion matrix\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(val_conf_matrix, annot=True, fmt='g', cmap='Blues', xticklabels=train_dataset.classes, yticklabels=train_dataset.classes)\n",
    "plt.xlabel('Predictions')\n",
    "plt.ylabel('Targets')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and test the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "layers = [\n",
    "    Dense(input_dim, 512),\n",
    "    Batchnorm(512),\n",
    "    ReLU(),\n",
    "    Dropout(0.5),\n",
    "    Dense(512, 256),\n",
    "    Batchnorm(256),\n",
    "    ReLU(),\n",
    "    Dropout(0.5),\n",
    "    Dense(256, output_dim),\n",
    "    SoftMax()\n",
    "]\n",
    "\n",
    "# test the model\n",
    "accuracy, precision, recall, f1, conf_matrix = tester(\n",
    "    layers=layers,\n",
    "    path='model/model.pkl',\n",
    "    test_data=test_data,\n",
    "    test_labels=test_labels,\n",
    "    batch_size=64\n",
    ")\n",
    "\n",
    "# plot the confusion matrix\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='g', cmap='Blues', xticklabels=test_dataset.classes, yticklabels=test_dataset.classes)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
