{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelEncoder = LabelEncoder()\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.utils import resample\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MyLogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, num_iteration=10000, regularization='l2', lambda_=0.01):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iteration = num_iteration\n",
    "        self.regularization = regularization\n",
    "        self.lambda_ = lambda_ \n",
    "        self.theta = None\n",
    "        self.bias = None\n",
    "    \n",
    "    def __sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def __loss(self, h, y):\n",
    "        emperical_loss = (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
    "        complexity_penalty = 0\n",
    "\n",
    "        if self.regularization == 'l2':\n",
    "            complexity_penalty = self.lambda_ * (self.theta ** 2).mean()\n",
    "        elif self.regularization == 'l1':\n",
    "            complexity_penalty = self.lambda_ * abs(self.theta).mean()\n",
    "        else:\n",
    "            complexity_penalty = 0\n",
    "\n",
    "        return emperical_loss + complexity_penalty \n",
    "\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "\n",
    "        \n",
    "        self.theta = np.zeros(X.shape[1])\n",
    "\n",
    "        \n",
    "        self.bias = 0\n",
    "\n",
    "\n",
    "        # print(\"--------------------------------------\")\n",
    "        for i in range(self.num_iteration):\n",
    "            z = np.dot(X, self.theta) + self.bias\n",
    "            h = self.__sigmoid(z)\n",
    "            gradient = np.dot(X.T, (h - y)) / y.size\n",
    "            gradient_bias = np.sum(h - y) / y.size\n",
    "            \n",
    "            if self.regularization == 'l2':\n",
    "                self.theta -= self.learning_rate * (gradient + self.lambda_ * self.theta)\n",
    "            elif self.regularization == 'l1':\n",
    "                self.theta -= self.learning_rate * (gradient + self.lambda_ * np.sign(self.theta))\n",
    "            else:\n",
    "                self.theta -= self.learning_rate * gradient\n",
    "            \n",
    "            self.bias -= self.learning_rate * gradient_bias\n",
    "\n",
    "            if i % 1000 == 0:\n",
    "                z = np.dot(X, self.theta) + self.bias\n",
    "                h = self.__sigmoid(z)\n",
    "                # print(f'loss: {self.__loss(h, y)} \\t')\n",
    "        \n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = np.array(X)\n",
    "        z = np.dot(X, self.theta) + self.bias\n",
    "        return np.array([1 if i > 0.5 else 0 for i in self.__sigmoid(z)])\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        X = np.array(X)\n",
    "        z = np.dot(X, self.theta) + self.bias\n",
    "        return self.__sigmoid(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_dataset(dataset_path):\n",
    "    # Load the datset\n",
    "    dataset = pd.read_csv(dataset_path)\n",
    "    return dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_features_target(dataframe, target_column):\n",
    "    X = dataframe.drop(target_column, axis=1)\n",
    "    y = dataframe[target_column]\n",
    "    return X, y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_numeric_missing_values(dataframe):\n",
    "    numerical_cols = dataframe.select_dtypes(include=['number']).columns\n",
    "    dataframe[numerical_cols] = dataframe[numerical_cols].fillna(dataframe[numerical_cols].mean())\n",
    "    return dataframe\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_categorical_missing_values(dataframe):\n",
    "    non_numerical_cols = dataframe.select_dtypes(exclude=['number']).columns\n",
    "    for col in non_numerical_cols:\n",
    "        dataframe[col] = dataframe[col].fillna(dataframe[col].mode()[0])\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_rows_with_missing_target(dataframe, target_column):\n",
    "    dataframe = dataframe.dropna(subset=[target_column])\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_non_numeric_to_categotical(features):\n",
    "    non_numeric_columns = features.select_dtypes(exclude=['number']).columns\n",
    "    for column in non_numeric_columns:\n",
    "        features[column] = pd.Categorical(features[column])\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_encoding(features):\n",
    "    # if there are two uinque values in a column, then perform label encoding\n",
    "    # if there are more than two unique values in a categorical column, then perform one hot encoding\n",
    "    non_categorical_columns = []\n",
    "    for column in features.columns:\n",
    "        if features[column].dtype.name == 'category':\n",
    "            if len(features[column].unique()) == 2:\n",
    "                features[column] = labelEncoder.fit_transform(features[column])\n",
    "        else:\n",
    "            non_categorical_columns.append(column)\n",
    "            \n",
    "    features = pd.get_dummies(features).astype('int64')\n",
    "\n",
    "    return features, non_categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_features(features, non_categorical_columns, scaling_strategy='Standard'):\n",
    "    if scaling_strategy == 'Standard':\n",
    "        scaler = StandardScaler()\n",
    "    elif scaling_strategy == 'MinMax':\n",
    "        scaler = MinMaxScaler()\n",
    "    else:\n",
    "        return \"Invalid Scaling Strategy\"\n",
    "    \n",
    "    \n",
    "    features[non_categorical_columns] = scaler.fit_transform(features[non_categorical_columns])\n",
    "    return features\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_n_features_based_on_correlation(features, target, n):\n",
    "    correlation = features.corrwith(target)\n",
    "    correlation = correlation.sort_values(ascending=False, key=lambda x: abs(x))\n",
    "    # print(correlation)\n",
    "    \n",
    "    if(n > len(correlation)):\n",
    "        n = len(correlation)\n",
    "\n",
    "    selected_features = correlation.head(n).index\n",
    "    features_selected = features[selected_features]\n",
    "\n",
    "    return features_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(dataframe, target_column, scaling_strategy='Standard', top_n_features=50, columns_to_scale=None):\n",
    "\n",
    "    print(\"Dataframe shape: \", dataframe.shape)\n",
    "    print()\n",
    "\n",
    "    print(\"Before removing rows with missing target values: \", dataframe.shape)\n",
    "    # Remove rows with missing target values\n",
    "    dataframe =  remove_rows_with_missing_target(dataframe, target_column)\n",
    "    print(\"After removing rows with missing target values: \", dataframe.shape)\n",
    "    print()\n",
    "\n",
    "    print(\"Before dropping duplicate rows: \", dataframe.shape)\n",
    "    # Drop duplicate rows\n",
    "    dataframe.drop_duplicates(inplace=True)\n",
    "    print(\"After dropping duplicate rows: \", dataframe.shape)\n",
    "    print()\n",
    "\n",
    "    print(\"Before replacing numeric null values: \", dataframe.isnull().sum().sum())\n",
    "    # Replace missing values in the numeric columns with the mean \n",
    "    dataframe = replace_numeric_missing_values(dataframe)\n",
    "    print(\"After replacing numeric null values \", dataframe.isnull().sum().sum())\n",
    "    print()\n",
    "\n",
    "    print(\"Before replacing categorical null values: \", dataframe.isnull().sum().sum())\n",
    "    # Replace missing values in the categorical columns with the mode\n",
    "    dataframe = replace_categorical_missing_values(dataframe)\n",
    "    print(\"After replacing categorical null values: \", dataframe.isnull().sum().sum())\n",
    "    print()\n",
    "\n",
    "\n",
    "    # Split the dataset into features and target\n",
    "    features, target = split_features_target(dataframe, target_column)\n",
    "\n",
    "    \n",
    "\n",
    "    print(\"Before encoding: \", features.shape)  \n",
    "\n",
    "    # Label Encode the target column\n",
    "    target = labelEncoder.fit_transform(target)\n",
    "    \n",
    "    # convert non-numeric columns to categorical\n",
    "    features = convert_non_numeric_to_categotical(features)\n",
    "    \n",
    "    # Perform encoding\n",
    "    # Save the non-categorical columns for scaling\n",
    "    features, non_categorical_columns = perform_encoding(features)\n",
    "\n",
    "    print(\"After encoding: \", features.shape)\n",
    "    print()\n",
    "\n",
    "    if(columns_to_scale is not None):\n",
    "        non_categorical_columns = columns_to_scale\n",
    "    \n",
    "    # Scale the features\n",
    "    features = scale_features(features, non_categorical_columns, scaling_strategy)\n",
    "    \n",
    "\n",
    "    target = pd.DataFrame(target, columns=[target_column])\n",
    "    target_series = target[target_column]\n",
    "\n",
    "    \n",
    "    # Select top n features based on correlation\n",
    "    features = top_n_features_based_on_correlation(features, target_series, top_n_features)\n",
    "\n",
    "    return features, target_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_score(y_test, y_pred, pred_proba):\n",
    "    # Calculate Accuracy Sensitivity Specificity Precision F1-score AUROC AUPR\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    sensitivity = recall_score(y_test, y_pred)\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "\n",
    "    specificity = tn / (tn + fp)\n",
    "\n",
    "    precision = precision_score(y_test, y_pred, zero_division=1)\n",
    "\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    auroc = roc_auc_score(y_test, pred_proba)\n",
    "\n",
    "    aupr = average_precision_score(y_test, pred_proba)\n",
    "\n",
    "    return accuracy, sensitivity, specificity, precision, f1, auroc, aupr\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def draw_violin_plots(base_metrics):\n",
    "    # Convert the base_metrics dictionary to a DataFrame for easy plotting with seaborn\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'Accuracy': base_metrics['accuracy'],\n",
    "        'Sensitivity': base_metrics['sensitivity'],\n",
    "        'Specificity': base_metrics['specificity'],\n",
    "        'Precision': base_metrics['precision'],\n",
    "        'F1-score': base_metrics['f1'],\n",
    "        'AUROC': base_metrics['auroc'],\n",
    "        'AUPR': base_metrics['aupr']\n",
    "    })\n",
    "\n",
    "    # Melt the DataFrame into long format suitable for seaborn\n",
    "    metrics_df_melted = metrics_df.melt(var_name=\"Metric\", value_name=\"Score\")\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "\n",
    "    colors = sns.color_palette(\"husl\", len(metrics_df.columns))\n",
    "\n",
    "    \n",
    "    sns.violinplot(x=\"Metric\", y=\"Score\", hue=\"Metric\", data=metrics_df_melted, palette=colors, legend=False)\n",
    "\n",
    "    plt.title(\"Performance Metrics\", fontsize=16)\n",
    "\n",
    "  \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the mean and standard deviation of the metrics\n",
    "def calculate_mean_std(base_metrics):\n",
    "    mean_std = {}\n",
    "    for metric in base_metrics:\n",
    "        metric_values = np.array(base_metrics[metric])\n",
    "        mean_std[metric] = {'mean': metric_values.mean(), 'std': metric_values.std()}\n",
    "    return mean_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacking_ensemble(features, target, val_size=0.2, test_size=0.2, n_base_classifiers=9, lr=0.01, num_iteration=10000, regularization='l2', lambda_=0.01, features_train=None, target_train=None, features_test=None, target_test=None):\n",
    "    \n",
    "\n",
    "    X_train = None\n",
    "    y_train = None\n",
    "    X_test = None\n",
    "    y_test = None\n",
    "    \n",
    "    if features_train is not None and target_train is not None and features_test is not None and target_test is not None:\n",
    "        X_train = features_train\n",
    "        y_train = target_train\n",
    "        X_test = features_test\n",
    "        y_test = target_test\n",
    "    else:\n",
    "        # Split the dataset into training and testing\n",
    "        X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=test_size, random_state=42)\n",
    "\n",
    "\n",
    "    # Split the training data into training and validation\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=val_size, random_state=42)\n",
    "\n",
    "    # Save the predictions of each model\n",
    "    predictions = []\n",
    "\n",
    "    # Save the models\n",
    "    base_models = []\n",
    "\n",
    "    # draw violin plots for each performance metric(Accuracy, Sensitivity, Specificity, Precision ,F1-score, AUROC, AUPR) for the 9 bagging LR learners.\n",
    "    # Save performance metrics for each base classifier\n",
    "    base_metrics = {\n",
    "        'accuracy': [],\n",
    "        'sensitivity': [],\n",
    "        'specificity': [],\n",
    "        'precision': [],\n",
    "        'f1': [],\n",
    "        'auroc': [],\n",
    "        'aupr': []\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "    for i in range(n_base_classifiers):\n",
    "        # Create an instance of the Logistic Regression model\n",
    "        model = MyLogisticRegression(learning_rate=lr, num_iteration=num_iteration, regularization=regularization, lambda_=lambda_)\n",
    "        # model = LogisticRegression()\n",
    "\n",
    "        # Sample with replacement from 100% of the training data with resample function\n",
    "        X_train_sampled, y_train_sampled = resample(X_train, y_train, replace=True, random_state=i, n_samples=len(X_train))\n",
    "\n",
    "        # Train the model\n",
    "        model.fit(X_train_sampled, y_train_sampled)\n",
    "\n",
    "        # Save the model\n",
    "        base_models.append(model)\n",
    "\n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_val)\n",
    "\n",
    "        # Save the predictions\n",
    "        predictions.append(y_pred)\n",
    "\n",
    "\n",
    "    # Convert the list of predictions to a numpy array\n",
    "    predictions = np.array(predictions)\n",
    "\n",
    "    # Shape = (n_samples, n_base_classifiers)\n",
    "    # Add these predictions as features to the validation set   \n",
    "    X_val = np.hstack((X_val, predictions.T))\n",
    "   \n",
    "\n",
    "    # Train a meta-classifier on the validation set\n",
    "    meta_classifier = MyLogisticRegression(learning_rate=lr, num_iteration=num_iteration, regularization=regularization, lambda_=lambda_)\n",
    "    # meta_classifier = LogisticRegression()\n",
    "    meta_classifier.fit(X_val, y_val)\n",
    "\n",
    "    # Save the predictions of each model on the test set\n",
    "    test_predictions = []\n",
    "    test_predictions_proba = []\n",
    "\n",
    "    for model in base_models:\n",
    "        pred = model.predict(X_test)\n",
    "        test_predictions.append(pred)\n",
    "\n",
    "        # Save the prediction probabilities\n",
    "        pred_proba = model.predict_proba(X_test)\n",
    "        test_predictions_proba.append(pred_proba)\n",
    "\n",
    "    \n",
    "        accuracy, sensitivity, specificity, precision, f1, auroc, aupr = calculate_score(y_test, pred, pred_proba)\n",
    "\n",
    "        \n",
    "        base_metrics['accuracy'].append(accuracy)\n",
    "        base_metrics['sensitivity'].append(sensitivity)\n",
    "        base_metrics['specificity'].append(specificity)\n",
    "        base_metrics['precision'].append(precision)\n",
    "        base_metrics['f1'].append(f1)\n",
    "        base_metrics['auroc'].append(auroc)\n",
    "        base_metrics['aupr'].append(aupr)\n",
    "\n",
    "\n",
    "    # Calculate the majority voting predictions\n",
    "    test_predictions_proba_m = np.array(test_predictions_proba)\n",
    "    test_predictions_proba_m = test_predictions_proba_m.mean(axis=0)\n",
    "    \n",
    "    test_predictions_m = np.array(test_predictions)\n",
    "    test_predictions_m = test_predictions_m.T\n",
    "    majority_voting_pred = []\n",
    "    for i in range(test_predictions_m.shape[0]):\n",
    "        majority_voting_pred.append(np.bincount(test_predictions_m[i]).argmax())\n",
    "    accuracy_MV, sensitivity_MV, specificity_MV, precision_MV, f1_MV, auroc_MV, aupr_MV = calculate_score(y_test, majority_voting_pred, test_predictions_proba_m)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    # Convert the list of predictions to a numpy array\n",
    "    test_predictions = np.array(test_predictions)\n",
    "\n",
    "    # Shape = (n_samples, n_base_classifiers)\n",
    "    # Add these predictions as features to the test set\n",
    "    X_test = np.hstack((X_test, test_predictions.T))\n",
    "\n",
    "    # Make predictions using the meta-classifier\n",
    "    meta_classifier_pred = meta_classifier.predict(X_test)\n",
    "    meta_classifier_pred_proba = meta_classifier.predict_proba(X_test)\n",
    "\n",
    "    # Calculate the scores\n",
    "    accuracy_SE, sensitivity_SE, specificity_SE, precision_SE, f1_SE, auroc_SE, aupr_SE = calculate_score(y_test, meta_classifier_pred, meta_classifier_pred_proba)\n",
    "\n",
    "    return accuracy_SE, sensitivity_SE, specificity_SE, precision_SE, f1_SE, auroc_SE, aupr_SE, accuracy_MV, sensitivity_MV, specificity_MV, precision_MV, f1_MV, auroc_MV, aupr_MV, base_metrics\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for table creation\n",
    "def create_table(data):\n",
    "    headers = [\"Model\", \"Accuracy\", \"Sensitivity\", \"Specificity\", \"Precision\", \"F1\", \"AUROC\", \"AUPR\"]\n",
    "    table = tabulate(data, headers, tablefmt=\"pretty\")\n",
    "    print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_dataset1_churn(path = 'WA_Fn-UseC_-Telco-Customer-Churn.csv' , top_n_features=100, val_size=0.2, test_size=0.2, n_base_classifiers=9, lr=0.01, num_iteration=10000, regularization='l2', lambda_=0.01):\n",
    "    dataframe = load_dataset(path)\n",
    "\n",
    "    # Replace all the ' ' with np.nan\n",
    "    dataframe.replace(' ', np.nan, inplace=True)\n",
    "\n",
    "    # Convert the 'TotalCharges' column to numeric\n",
    "    dataframe['TotalCharges'] = pd.to_numeric(dataframe['TotalCharges'], errors='coerce')\n",
    "\n",
    "    # Drop the 'customerID' column, cuz it is not useful\n",
    "    dataframe.drop('customerID', axis=1, inplace=True)\n",
    "\n",
    "    # preprocess the dataset: replace null values, encode categorical columns, scale the features\n",
    "    features, target = preprocess_dataset(dataframe, target_column='Churn', scaling_strategy='Standard', top_n_features=top_n_features)\n",
    "\n",
    "\n",
    "    accuracy_SE, sensitivity_SE, specificity_SE, precision_SE, f1_SE, auroc_SE, aupr_SE, accuracy_MV, sensitivity_MV, specificity_MV, precision_MV, f1_MV, auroc_MV, aupr_MV, base_metrics = stacking_ensemble(features, target, val_size=val_size, test_size = test_size, n_base_classifiers=n_base_classifiers, regularization=regularization, lambda_=lambda_, lr=lr, num_iteration=num_iteration)\n",
    "\n",
    "    # Calculate mean and standard deviation for each metric\n",
    "    mean_std = calculate_mean_std(base_metrics)\n",
    "\n",
    "\n",
    "    # Format the data so that it shows as 'mean +- std' in the table\n",
    "    data = [\n",
    "        [\"Logistic Regression\", f\"{mean_std['accuracy']['mean']:f} ± {mean_std['accuracy']['std']:f}\",  f\"{mean_std['sensitivity']['mean']:f} ± {mean_std['sensitivity']['std']:f}\", f\"{mean_std['specificity']['mean']:f} ± {mean_std['specificity']['std']:f}\", f\"{mean_std['precision']['mean']:f} ± {mean_std['precision']['std']:f}\", f\"{mean_std['f1']['mean']:f} ± {mean_std['f1']['std']:f}\", f\"{mean_std['auroc']['mean']:f} ± {mean_std['auroc']['std']:f}\", f\"{mean_std['aupr']['mean']:f} ± {mean_std['aupr']['std']:f}\"],\n",
    "        [\"Majority Voting\", accuracy_MV, sensitivity_MV, specificity_MV, precision_MV, f1_MV, auroc_MV, aupr_MV],\n",
    "        [\"Stacking Ensemble\", accuracy_SE, sensitivity_SE, specificity_SE, precision_SE, f1_SE, auroc_SE, aupr_SE]\n",
    "    ]\n",
    "\n",
    "    create_table(data)\n",
    "\n",
    "    # Draw violin plots\n",
    "    draw_violin_plots(base_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_dataset2_adult(path_data = 'adult.data', path_test = 'adult.test', top_n_features=200, val_size=0.2, test_size=0.2, n_base_classifiers=9, lr=0.01, num_iteration=10000, regularization='l2', lambda_=0.01):\n",
    "    # load adult.data dataset\n",
    "    headers = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income']\n",
    "    dataframe_train = pd.read_csv(path_data, names=headers, engine='python')\n",
    "\n",
    "    # load adult.test dataset\n",
    "    dataframe_test = pd.read_csv(path_test, names=headers, engine='python', skiprows=1)\n",
    "\n",
    "\n",
    "    # Replace all the ' ?' with np.nan\n",
    "    dataframe_train.replace(' ?', np.nan, inplace=True)\n",
    "    dataframe_test.replace(' ?', np.nan, inplace=True)\n",
    "\n",
    "\n",
    "    # replace <=50K. with <=50K and >50K. with >50K\n",
    "    dataframe_test['income'] = dataframe_test['income'].replace({' <=50K.': ' <=50K', ' >50K.': ' >50K'})\n",
    "\n",
    "\n",
    "    train_len = len(dataframe_train)\n",
    "    test_len = len(dataframe_test)\n",
    "    ratio = train_len / (train_len + test_len)\n",
    "\n",
    "\n",
    "    dataframe = pd.concat([dataframe_train, dataframe_test])\n",
    "\n",
    "    #preprocess the dataset: replace null values, encode categorical columns, scale the features and select top 200 features\n",
    "    features, target = preprocess_dataset(dataframe, target_column='income', scaling_strategy='Standard', top_n_features=top_n_features)\n",
    "\n",
    "    # Split the features and target back into training and testing data according to the ratio\n",
    "    train_len = int(ratio * len(features))\n",
    "    features_train = features[:train_len]\n",
    "    target_train = target[:train_len]\n",
    "\n",
    "    features_test = features[train_len:]\n",
    "    target_test = target[train_len:]\n",
    "\n",
    "    # Run the stacking ensemble\n",
    "    accuracy_SE, sensitivity_SE, specificity_SE, precision_SE, f1_SE, auroc_SE, aupr_SE, accuracy_MV, sensitivity_MV, specificity_MV, precision_MV, f1_MV, auroc_MV, aupr_MV, base_metrics = stacking_ensemble(features, target, val_size=val_size, test_size=test_size, n_base_classifiers=n_base_classifiers, regularization=regularization, lambda_=lambda_, features_train=features_train, target_train=target_train, features_test=features_test, target_test=target_test, num_iteration=num_iteration, lr=lr)\n",
    "\n",
    "\n",
    "    # Calculate mean and standard deviation for each metric\n",
    "    mean_std = calculate_mean_std(base_metrics)\n",
    "\n",
    "\n",
    "\n",
    "    data = [\n",
    "        [\"Logistic Regression\", f\"{mean_std['accuracy']['mean']:f} ± {mean_std['accuracy']['std']:f}\",  f\"{mean_std['sensitivity']['mean']:f} ± {mean_std['sensitivity']['std']:f}\", f\"{mean_std['specificity']['mean']:f} ± {mean_std['specificity']['std']:f}\", f\"{mean_std['precision']['mean']:f} ± {mean_std['precision']['std']:f}\", f\"{mean_std['f1']['mean']:f} ± {mean_std['f1']['std']:f}\", f\"{mean_std['auroc']['mean']:f} ± {mean_std['auroc']['std']:f}\", f\"{mean_std['aupr']['mean']:f} ± {mean_std['aupr']['std']:f}\"],\n",
    "        [\"Majority Voting\", accuracy_MV, sensitivity_MV, specificity_MV, precision_MV, f1_MV, auroc_MV, aupr_MV],\n",
    "        [\"Stacking Ensemble\", accuracy_SE, sensitivity_SE, specificity_SE, precision_SE, f1_SE, auroc_SE, aupr_SE]\n",
    "    ]\n",
    "\n",
    "    create_table(data)\n",
    "\n",
    "    # Draw violin plots\n",
    "    draw_violin_plots(base_metrics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_dataset3_creditcardfraud(path='creditcardfraud.csv',top_n_features=100, val_size=0.2, test_size=0.2, n_base_classifiers=9, lr=0.01, num_iteration=10000, regularization='l2', lambda_=0.01):\n",
    "    dataframe = load_dataset(path)\n",
    "    # count the number of '1' and '0' in the 'Class' column\n",
    "    print(dataframe['Class'].value_counts())\n",
    "    print()\n",
    "\n",
    "    # (randomly selected 20000 negative samples + all positive samples)\n",
    "    dataframe = dataframe.sample(frac=1, random_state=42)\n",
    "    negative_samples = dataframe[dataframe['Class'] == 0].head(20000)\n",
    "    positive_samples = dataframe[dataframe['Class'] == 1]\n",
    "    dataframe = pd.concat([negative_samples, positive_samples])\n",
    "\n",
    "    # preprocess the dataset: replace null values, encode categorical columns, scale the features, select top n features based on correlation\n",
    "    features, target = preprocess_dataset(dataframe, target_column='Class', scaling_strategy='Standard', top_n_features=top_n_features, columns_to_scale=['Time', 'Amount'])\n",
    "\n",
    "    accuracy_SE, sensitivity_SE, specificity_SE, precision_SE, f1_SE, auroc_SE, aupr_SE, accuracy_MV, sensitivity_MV, specificity_MV, precision_MV, f1_MV, auroc_MV, aupr_MV, base_metrics = stacking_ensemble(features, target, val_size=val_size, test_size=test_size, n_base_classifiers=n_base_classifiers, regularization=regularization, lambda_=lambda_, num_iteration=num_iteration, lr=lr)\n",
    "\n",
    "    # Calculate mean and standard deviation for each metric\n",
    "    mean_std = calculate_mean_std(base_metrics)\n",
    "\n",
    "\n",
    "    # Format the data so that it shows as 'mean +- std' in the table\n",
    "    data = [\n",
    "        [\"Logistic Regression\", f\"{mean_std['accuracy']['mean']:f} ± {mean_std['accuracy']['std']:f}\",  f\"{mean_std['sensitivity']['mean']:f} ± {mean_std['sensitivity']['std']:f}\", f\"{mean_std['specificity']['mean']:f} ± {mean_std['specificity']['std']:f}\", f\"{mean_std['precision']['mean']:f} ± {mean_std['precision']['std']:f}\", f\"{mean_std['f1']['mean']:f} ± {mean_std['f1']['std']:f}\", f\"{mean_std['auroc']['mean']:f} ± {mean_std['auroc']['std']:f}\", f\"{mean_std['aupr']['mean']:f} ± {mean_std['aupr']['std']:f}\"],\n",
    "        [\"Majority Voting\", accuracy_MV, sensitivity_MV, specificity_MV, precision_MV, f1_MV, auroc_MV, aupr_MV],\n",
    "        [\"Stacking Ensemble\", accuracy_SE, sensitivity_SE, specificity_SE, precision_SE, f1_SE, auroc_SE, aupr_SE]\n",
    "    ]\n",
    "\n",
    "    create_table(data)\n",
    "\n",
    "    # Draw violin plots\n",
    "    draw_violin_plots(base_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><i>Telco Customer Churn (Dataset-1) </i> </h3>\n",
    "<b><i> Comment out the later portion to run on dataset-1: Churn data </i></b><br>\n",
    "Link: https://www.kaggle.com/datasets/blastchar/telco-customer-churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_dataset1_churn(\n",
    "    path = 'WA_Fn-UseC_-Telco-Customer-Churn.csv',\n",
    "    top_n_features=20,\n",
    "    val_size=0.2,\n",
    "    test_size=0.2,\n",
    "    n_base_classifiers=9,\n",
    "    lr=0.01,\n",
    "    num_iteration=10000,\n",
    "    regularization='l2',\n",
    "    lambda_=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><i>Adult (Dataset-2): Predict whether income exceeds $50K/yr based on census data </i> </h3>\n",
    "\n",
    "<b><i> Comment out the later portion to run on dataset-2: Adult data </i></b><br>\n",
    "Link: https://archive.ics.uci.edu/dataset/2/adult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_dataset2_adult(\n",
    "    path_data = 'adult.data',\n",
    "    path_test = 'adult.test',\n",
    "    top_n_features=20,\n",
    "    n_base_classifiers=9,\n",
    "    lr=0.01,\n",
    "    num_iteration=10000,\n",
    "    regularization='l2',\n",
    "    lambda_=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><i>Credit Card Fraud Detection (Dataset-3) </i> </h3>\n",
    "\n",
    "<b><i> Comment out the later portion to run on dataset-3: Credit Card Fraud Detection </i></b><br>\n",
    "Link: https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_dataset3_creditcardfraud(\n",
    "    path='creditcardfraud.csv',\n",
    "    top_n_features=20,\n",
    "    val_size=0.2,\n",
    "    test_size=0.2,\n",
    "    n_base_classifiers=9,\n",
    "    lr=0.01,\n",
    "    num_iteration=10000,\n",
    "    regularization='l2',\n",
    "    lambda_=0.01\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
